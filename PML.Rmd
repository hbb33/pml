# Data preparation
As there are a large numbe of columns with N/A data and as there are not used in the testing.csv file, I have taken the option to simply remove these columns.
To do that I simply imported the CSV file into Excel and manually deleted the columns. I then save the new file
in a new CSV file (called Mytraining1.csv)

# R prerapation
 
 I have chosen to use the Random Forest from the caret package for the exercise.
 So in the R code:
 * I load the caret package
 
  
```
library(caret)
```
 
# Data  prerapation
 
* I then load my data with the read.csv R function. As there are a large numbe of columns with N/A data, I use na.strings option to identify the missing data
 * I remove the first six columns that does  contains "admin" data not useful for  training 
 * I then remove the NA data
 * I then split my data into a training and a testing set (70/30 split)
 
```
mydata <- read.csv(file="pml-training.csv",head=TRUE,sep=",",na.strings=c("NA",""))
mydata<-mydata[,7:160]
mostly_data<-apply(!is.na(mydata),2,sum)>19621
mydata<-mydata[,mostly_data]

inTrain <- createDataPartition(y=mydata$classe,p=0.7, list=FALSE) 
training <- mydata[inTrain,]
testing <- mydata[-inTrain,]
```

# Training phase
I use the Random Forest method for the training phase. The R code is:
```
modFit <- train(classe~ .,data=training,method="rf",trControl=trainControl(method="cv",number=5),prox=TRUE,allowParallel=TRUE)
modFit
```

The training gives good results with an accuracy of 99.67%:
```
Random Forest 

13737 samples
   53 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (5 fold) 

Summary of sample sizes: 10991, 10990, 10988, 10989, 10990 

Resampling results across tuning parameters:

  mtry  Accuracy   Kappa      Accuracy SD   Kappa SD   
   2    0.9931569  0.9913430  0.0010750283  0.001360236
  27    0.9967242  0.9958564  0.0007721394  0.000976617
  53    0.9910456  0.9886725  0.0031075881  0.003931862

Accuracy was used to select the optimal model using  the largest value.
The final value used for the model was mtry = 27. 
```
# Validation with testing set
I validate the accuracy of the model using the testing set. The R code to do that is:
```
pred <- predict(modFit, testing)
testing$predRight <- pred==testing$classe
table (pred, testing$classe)
```
The model gives good result with an accuracy of 99.6% ( 21 misclassification over 5885) on the testing set as shown below:
```   
pred    A    B    C    D    E
   A 1674    4    0    0    0
   B    0 1130    1    0    0
   C    0    4 1025    3    0
   D    0    1    0  958    5
   E    0    0    0    3 1077
```

# Prediting classication for the exercise
I choose to keep the model validated below and apply it to the data to be predicted. The R code is:
```
mydata1 <- read.csv(file="pml-testing.csv",head=TRUE,sep=",")
pred1 <- predict(modFit, mydata1)
pred1
```

The resulting classication is:
```
pred1
 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E
```

This result have been submitted on Coursera site and all successfully validated
